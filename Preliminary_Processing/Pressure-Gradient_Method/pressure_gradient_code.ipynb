{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26cfd2af-5e7a-4ce8-be65-bb9ba1e14840",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import modules\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as dt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1145e33d-0ea9-49a9-bc1d-6086711ce6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========Define Key Functions===============================================\n",
    "def generate_file_list(data_dir, fname, numlines = 0):\n",
    "    \n",
    "    '''This function opens a text file containing a list of filenames to process,\n",
    "    reads the lines, appends the directory path to the start of each filename, \n",
    "    and returns the data as a list of stings with a full-path to each file to\n",
    "    be processed.'''\n",
    "    \n",
    "    #Open a file object\n",
    "    file_var = open(data_dir + fname, 'r')\n",
    "    \n",
    "    # Call the readlines method of the file object to read the data from the \n",
    "    # file object, where each line of the file is a new item in the list. The \n",
    "    # readlines method returns a list of strings. \n",
    "    file_list = file_var.readlines()\n",
    "    \n",
    "    # Remove the newline character and append the directory path to the start \n",
    "    # of each filename\n",
    "    for line in range(len(file_list)):\n",
    "        \n",
    "        file_list[line] = file_list[line].rstrip()\n",
    "        file_list[line] = data_dir + file_list[line]\n",
    "    \n",
    "    return file_list[:numlines]\n",
    "    \n",
    "def netcdf_time_scale(y,m,d,h,y0):\n",
    "    \n",
    "    '''This function takes a date in y,m,d,h format and returns the value of the\n",
    "    hours since jan 1, y0 (the time convention of reanalysis). Inputs are integers. \n",
    "    Output is a float.''' \n",
    "    \n",
    "    d0 = datetime(y0,1,1,0) # the datetime object representing 12am, jan 1 of the baseline year of the reanalysis\n",
    "    d = datetime(y,m,d,h) # the datetime object for the day and time being tested\n",
    "    \n",
    "    t = 24 * (dt.date2num(d) - dt.date2num(d0)) # number of hours since 12am, jan 1 of the baseline year\n",
    "    # dt.date2num gives the number of days since the python datetime time origin. \n",
    "    \n",
    "    return t\n",
    "\n",
    "def date_from_netcdftime(t_netcdf, y0):\n",
    "    \n",
    "    '''This function takes a datetime64 object and converts it to a date in y,m,d,h format. Output is a tuple. '''\n",
    "    '''\n",
    "    d0 = datetime(y0,1,1,0) # the datetime object representing 12am, jan 1 of the baseline year of the reanalysis\n",
    "    \n",
    "    d = dt.date2num(t_netcdf) / 24 + dt.date2num(d0) # number of days since jan 1 of the python datetime baseline year\n",
    "    \n",
    "    date = dt.num2date(d)\n",
    "    '''\n",
    "    date = pd.Timestamp(t_netcdf)\n",
    "    \n",
    "    y = date.year\n",
    "    m = date.month\n",
    "    d = date.day\n",
    "    h = date.hour\n",
    "    \n",
    "    return y,m,d,h\n",
    "\n",
    "def horizontal_distance(low1, low2):\n",
    "    \n",
    "    '''This function takes a pair of (lat,lon) coordinates, and computes the linear\n",
    "    distance between them. Output is a float. '''\n",
    "    \n",
    "    lat1 = low1[0]\n",
    "    lat2 = low2[0]\n",
    "    \n",
    "    lon1 = low1[1]\n",
    "    lon2 = low2[1]\n",
    "    \n",
    "    lat_distance = abs(lat2 - lat1)\n",
    "    \n",
    "    # This part handles the situation where the two lows are close on either\n",
    "    # side of the line of 0 degrees longitude. \n",
    "    lon_dist1 = abs(lon2 - lon1)\n",
    "    lon_dist2 = 360 - abs(lon2 - lon1)\n",
    "    lon_distance = np.minimum(lon_dist1, lon_dist2) # np.minimum is important\n",
    "    # here so this function can be vectorised across a whole array of low centers.\n",
    "    \n",
    "    distance = (lat_distance ** 2 + lon_distance ** 2) ** 0.5\n",
    "    \n",
    "    return distance\n",
    "    \n",
    "def  low_center_test(field, threshold, a, b):\n",
    "    \n",
    "    '''This function takes as its input an array of numbers, and tests if the\n",
    "    cell [y,x] is the center of a local low by a minimum threshold. The cell\n",
    "    [y,x] is identified as a low center if its value is lower than the value of\n",
    "    each of the surrounding 8 cells at a distance of 2.5 degrees lat/lon, as well\n",
    "    as lower than the value of each of the surrounding 8 cells at a distance of\n",
    "    5 degrees lat/lon by a minimum difference threshold. The diagonal difference \n",
    "    checks are scaled by a factor of 2**0.5 by Pythagoras. The output is a tuple\n",
    "    of a boolean of whether the cell passes the low test or not, as well as \n",
    "    the values of the central cell value and the test 1 and test 2 mean values.'''\n",
    "    \n",
    "    # Get the center of an arbitrarily-shaped box\n",
    "    x = int((np.shape(field)[0] - 1) / 2)\n",
    "    y = x # center of a square has the same x and y value\n",
    "    \n",
    "    CELL = field[y,x] # gets the value of the field in the center of the box\n",
    "    \n",
    "    #Comparison between the central cell and the surrounding 8 cells\n",
    "    TEST1 = np.array([field[y-a,x-a]-CELL,field[y-a,x]-CELL,field[y-a,x+a]-CELL,field[y,x+a]-CELL,field[y+a,x+a]-CELL,field[y+a,x]-CELL,field[y+a,x-a]-CELL,field[y,x-a]-CELL],float)\n",
    "    TEST1[0::2] = TEST1[0::2]/(2**0.5)\n",
    "    \n",
    "    TEST2 = np.array([field[y-b,x-b]-CELL,field[y-b,x]-CELL,field[y-b,x+b]-CELL,field[y,x+b]-CELL,field[y+b,x+b]-CELL,field[y+b,x]-CELL,field[y+b,x-b]-CELL,field[y,x-b]-CELL],float)\n",
    "    TEST2[0::2] = TEST2[0::2]/(2**0.5)\n",
    "    \n",
    "    if (np.all(TEST1 >= 0)) and (np.all(TEST2 >= threshold)):\n",
    "    #if (np.all(TEST1 >= 0)) and (np.mean(TEST2) >= threshold):\n",
    "             \n",
    "        a = True\n",
    "        b = CELL\n",
    "        c = np.mean(TEST1)\n",
    "        d = np.mean(TEST2)\n",
    "   \n",
    "    else:\n",
    "        \n",
    "        a = False\n",
    "        b = np.nan\n",
    "        c = np.nan\n",
    "        d = np.nan\n",
    "        \n",
    "    return a, b, c, d\n",
    "\n",
    "def eliminate_secondary_lows(low_array, distance_threshold):\n",
    "    \n",
    "    '''This function removes minor low centers that are close to a deeper parent\n",
    "    low center in the same field. It does this by looping through each low center\n",
    "    identified in a field, and comparing it to each other low center in the same\n",
    "    field. If the two lows are within a given distance threshold, the shallower\n",
    "    low is overwritten with np.nan. Only the non-nan lows are then returned out\n",
    "    of the function. Output is a numpy array.'''\n",
    "    \n",
    "    # creates an array of each timestep with at least one low to loop through.\n",
    "    timerange = np.unique(low_array[:,5]).astype(int)\n",
    "    #print(timerange)\n",
    "    \n",
    "    for tstep in timerange: \n",
    "        \n",
    "        #print(np.shape(low_array[(low_array[:,5] == tstep), :]))\n",
    "        #print(tstep)\n",
    "        #if np.shape(low_array[(low_array[:,5] == tstep), :])[0] == 0:\n",
    "            #print('******==========ERROR IS HERE==========******')\n",
    "            #print(tstep)\n",
    "        \n",
    "        # extracts all of the rows in the low_array for the timestep being tested\n",
    "        ifield = (low_array[:,5] == tstep)\n",
    "        fieldlows = low_array[ifield, :]\n",
    "        \n",
    "        arraylen = np.shape(fieldlows)[0]\n",
    "        arrayloop = np.arange(0,arraylen,1)\n",
    "\n",
    "        if arraylen > 0:\n",
    "            \n",
    "            # Just a check to test if the deepest low entry has been blanked out. \n",
    "            # Will be compared to postfiltermin after the filtering process. \n",
    "            prefiltermin = np.nanmin(fieldlows[:,8])\n",
    "            \n",
    "            for a in arrayloop:\n",
    "                \n",
    "                low1 = fieldlows[a,:]\n",
    "                array_subloop = np.arange(a + 1, arraylen, 1)\n",
    "                \n",
    "                for b in array_subloop:\n",
    "                    \n",
    "                    low2 = fieldlows[b,:]\n",
    "                    \n",
    "                    # These conditional tests indicate whether two closed low \n",
    "                    # instances are within the spatial proximity of each other, and\n",
    "                    # which low center is the deepest. \n",
    "                    distance_test = horizontal_distance((low1[6], low1[7]), (low2[6], low2[7])) <= distance_threshold \n",
    "                    low1_shallower = (low1[8] > low2[8])\n",
    "                    low2_shallower = (low2[8] > low1[8])\n",
    "                    equal_lowtest = (low2[8] == low1[8])\n",
    "                    \n",
    "                    if distance_test and low1_shallower:\n",
    "                        \n",
    "                        low1[:] = np.nan\n",
    "                        \n",
    "                    elif distance_test and low2_shallower:\n",
    "                        \n",
    "                        low2[:] = np.nan\n",
    "                        \n",
    "                    elif distance_test and equal_lowtest:\n",
    "                        \n",
    "                        if low1[10] < low2[10]:\n",
    "                            \n",
    "                            low1[:] = np.nan\n",
    "                            \n",
    "                        elif low2[10] < low1[10]:\n",
    "                            \n",
    "                            low2[:] = np.nan\n",
    "                                         \n",
    "                        #else:\n",
    "                        \n",
    "                            # It may be helpful to receive an allert when two low\n",
    "                            # centers of equal depth are located in the array (which)\n",
    "                            # does occasionally happen. In this case both lows are\n",
    "                            # retained, though only one may persist through the tracking\n",
    "                            # stage. \n",
    "                            #print('FOUND TWO LOW CENTERS OF EQUAL DEPTH IN FIELD.')\n",
    "                            #print(low1)\n",
    "                            #print(low2)\n",
    "                    \n",
    "            postfiltermin = np.nanmin(fieldlows[:,8])\n",
    "            \n",
    "            if prefiltermin != postfiltermin:\n",
    "                \n",
    "                # This code checks if the deepest low in a timestep has been removed,\n",
    "                # due to some error in the process. It prints out the date of the\n",
    "                # data causing the error to help with diagnosing the cause of the error. \n",
    "                errordate = date_from_netcdftime(tstep, 1800)\n",
    "                print('Filtering error at tstep: ', errordate)\n",
    "                print('Prefiltermin: ', prefiltermin)\n",
    "                print('Postfiltermin: ', postfiltermin, '\\n')\n",
    "                print('fieldlows: ', fieldlows, '\\n')\n",
    "            \n",
    "            # The filtered low data (including np.nan rows) are written back into \n",
    "            # the base low array. \n",
    "            low_array[ifield,:] = fieldlows\n",
    "    \n",
    "    # This stage removes all the rows from the array that were blanked out with\n",
    "    # np.nan. \n",
    "    ir = np.isnan(low_array[:,0])\n",
    "    refined_lows = low_array[(ir == False),:]\n",
    "    \n",
    "    return refined_lows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfda9a0-15ad-4eff-a74e-b936d38fa457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==========SECTION 1 Identification Scheme Parameters===========================\n",
    "\n",
    "dataset = 'eraint' # ncep1 or eraint or era5\n",
    "\n",
    "save_path = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Input_Data/Nick_Lows/'\n",
    "\n",
    "#Time period setup\n",
    "year_range = (1979, 2017) #(start year,end year)\n",
    "startdate = (1,1) #(month,day)\n",
    "enddate = (12,31) #(month,day)\n",
    "\n",
    "#Spatial domain setup\n",
    "lat_range = (-21,-60) #Boundary of region in latitude\n",
    "lon_range = (-180, 179) #(-180,179) for ERA-Int, (-180, 178.5) for ERA5 #Boundary of region in longitude\n",
    "\n",
    "p_level = 50000 # 500 hPa for ERA5 and NCEP1, 50000 Pa for ERA-Int.  \n",
    "\n",
    "# Reanalysis data properties\n",
    "t_res = 6 #hrs\n",
    "spacial_res = 1 #degrees lat/lon # 1.5 for ERA5, 1 for ERA-Int\n",
    "\n",
    "#Cutoff low properties setup\n",
    "hgt_threshold = 0 #gpm. The minimum difference in pressure between the center\n",
    "            #square and each of the surrounding 8 squares to identify a closed \n",
    "            #low.\n",
    "            \n",
    "pg_threshold = 0  #hPa. Similar as above.\n",
    "\n",
    "#Filtering parameters\n",
    "\n",
    "filter_minor_lows = True\n",
    "\n",
    "low_filtering_spatial_threshold = 10 #For removing weaker low centers which are close to a \n",
    "            #deeper low center in the SAME pressure field. \n",
    "            #The maximum distance (in degees lat/lon) between two low centers \n",
    "            #in the same pressure field to consider them the same system.\n",
    "\n",
    "# THESE VARIABLES ARE ONLY USED FOR THE TRACKING STAGE. \n",
    "track_joining_spatial_threshold = 7.5 #For joining CONSECUTIVE low centers into tracks. The maximum \n",
    "            #distance (in degees lat/lon) between one low center in the \n",
    "            #'current' pressure field, and another low center in previous\n",
    "            #pressure fields to join them to the same track. \n",
    "            \n",
    "minimum_duration = 0 #hrs. The minimum duration for a low event to be included\n",
    "            # in the dataset, to eliminate short insignificant lows. \n",
    "\n",
    "if dataset == 'eraint':\n",
    "    \n",
    "    data_path = '/g/data/w40/nxg561/ERA-Int/'\n",
    "    hgt_file_prefix = 'z500.'\n",
    "    slp_file_prefix = 'slp.'\n",
    "    file_extension = '_rgd.nc'\n",
    "\n",
    "    time_y0 = 1900\n",
    "    \n",
    "    lat_variable_name = 'lat'\n",
    "    lon_variable_name = 'lon'\n",
    "    vertical_coordinate_name = 'lev' # 'level' for NCEP1, 'lev' for ERAInterim\n",
    "    time_variable_name = 'time'\n",
    "    gph_variable_name = 'z' # 'hgt' for NCEP1, 'z' for ERAInterim\n",
    "    slp_variable_name = 'psl'\n",
    "    \n",
    "elif dataset == 'ncep1':\n",
    "    \n",
    "    data_path = '/Volumes/ExternalHD/Large_Data/NCEP1/'\n",
    "    hgt_file_prefix = 'hgt.'\n",
    "    slp_file_prefix = 'slp.'\n",
    "    file_extension = '.nc'\n",
    "\n",
    "    time_y0 = 1800\n",
    "    \n",
    "    lat_variable_name = 'lat'\n",
    "    lon_variable_name = 'lon'\n",
    "    vertical_coordinate_name = 'level' # 'level' for NCEP1, 'lev' for ERAInterim\n",
    "    time_variable_name = 'time'\n",
    "    gph_variable_name = 'hgt' # 'hgt' for NCEP1, 'z' for ERAInterim\n",
    "    slp_variable_name = 'slp'\n",
    "    \n",
    "if dataset == 'era5':\n",
    "    \n",
    "    data_path = '/g/data/w40/nxg561/ERA5/'#'/Volumes/ExternalHD/Large_Data/ERA-Interim/Regridded/'\n",
    "    #file_list = 'era5_file_list_z_' + str(year_range[0]) + '.txt'\n",
    "\n",
    "    time_y0 = 1900\n",
    "    \n",
    "    lat_variable_name = 'lat'\n",
    "    lon_variable_name = 'lon'\n",
    "    vertical_coordinate_name = 'level' # 'level' for NCEP1, 'lev' for ERAInterim\n",
    "    time_variable_name = 'time'\n",
    "    gph_variable_name = 'z' # 'hgt' for NCEP1, 'z' for ERAInterim\n",
    "    slp_variable_name = 'psl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8a2bdc5-a9c0-41b0-9406-45bf3f29fc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==========SECTION 2 Utility variable setup=====================================\n",
    "\n",
    "#this section prealocates an array of zeros to store the data of the identified \n",
    "#lows in, with a number of rows for every time step in the whole time period.\n",
    "    \n",
    "tsteps = (netcdf_time_scale(year_range[1],12, 31, 18, time_y0) - netcdf_time_scale(year_range[0], 1, 1, 0, time_y0) ) + 1\n",
    "\n",
    "raw_lows_z = np.zeros((500 * int(tsteps), 11), dtype = float) #prealocates an array of \n",
    "            #zeros to store the lows in. I just guessed 500 x the total number of \n",
    "            #timesteps would be enough rows.\n",
    "\n",
    "# This part computes the conversion factor between the resolution of ncep1 (the \n",
    "# original dataset that this code was developed for) and the resolution of the \n",
    "# dataset being used here. \n",
    "scale_factor = 2.5 / spacial_res\n",
    "    \n",
    "a = math.floor(1 * scale_factor) # grid spaces for 1 grid space equivalent\n",
    "b = math.floor(2 * scale_factor) # grid spaces for 1 grid space equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5d832ea-7d2b-4f31-a7c1-517e64ae86ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==========SECTION 3 Data Input and Low Detection===============================\n",
    "\n",
    "# This part creates a list of filenames to open as a mf_dataset in xarray:\n",
    "\n",
    "#years = tuple(range(year_range[0],year_range[1]+1))\n",
    "\n",
    "#file_list = generate_file_list(data_path, file_list, numlines = 12)\n",
    "# Load all the reanalysis files in a multi-file dataset, and find all the locations\n",
    "# of local minima:\n",
    "\n",
    "#xr_obj = xr.open_mfdataset(file_list, combine = 'nested', concat_dim = 'time', engine = 'netcdf4')\n",
    "xr_obj = xr.open_mfdataset(data_path + 'z*.nc', combine = 'nested', concat_dim = 'time', engine = 'netcdf4')\n",
    "# Extract the data for the search domain, as well as the domain shifted one and\n",
    "# two grid spaces to the north and south. The east and west shifted data will\n",
    "# be obtained with a np.roll() function. These variables are an xarray data array. \n",
    "hgt_data = xr_obj[gph_variable_name].sel(lat = slice (lat_range[0], lat_range[1]), lon = slice (lon_range[0], lon_range[1]), lev = p_level)\n",
    "\n",
    "hgt_data_north = xr_obj[gph_variable_name].sel(lat = slice (lat_range[0] + a * spacial_res, lat_range[1] + a * spacial_res), lon = slice (lon_range[0], lon_range[1]), lev = p_level)\n",
    "hgt_data_north2 = xr_obj[gph_variable_name].sel(lat = slice (lat_range[0] + b * spacial_res, lat_range[1] + b * spacial_res), lon = slice (lon_range[0], lon_range[1]), lev = p_level)\n",
    "\n",
    "hgt_data_south = xr_obj[gph_variable_name].sel(lat = slice (lat_range[0] - a * spacial_res, lat_range[1] - a * spacial_res), lon = slice (lon_range[0], lon_range[1]), lev = p_level)\n",
    "hgt_data_south2 = xr_obj[gph_variable_name].sel(lat = slice (lat_range[0] - b * spacial_res, lat_range[1] - b * spacial_res), lon = slice (lon_range[0], lon_range[1]), lev = p_level)\n",
    "\n",
    "# Extract the numpy arrays to work with the actual data\n",
    "hgt_field = hgt_data.values\n",
    "hgt_field_north = hgt_data_north.values\n",
    "hgt_field_north2 = hgt_data_north2.values\n",
    "hgt_field_south = hgt_data_south.values\n",
    "hgt_field_south2 = hgt_data_south2.values\n",
    "\n",
    "# Some reanalysis datasets need to have geopotential height divided by g = 9.8 m/s^2. \n",
    "if dataset == 'era5' or dataset == 'eraint':\n",
    "        \n",
    "        hgt_field = hgt_field / 9.8\n",
    "        hgt_field_north = hgt_field_north / 9.8\n",
    "        hgt_field_north2 = hgt_field_north2 / 9.8\n",
    "        hgt_field_south = hgt_field_south / 9.8\n",
    "        hgt_field_south2 = hgt_field_south2 / 9.8\n",
    "        \n",
    "# Do the rolls to set up the east, west, north-east, north-west, south-east and\n",
    "# south-west shifted data here        \n",
    "hgt_field_west = np.roll(hgt_field, a, axis = 2)\n",
    "hgt_field_east = np.roll(hgt_field, -a, axis = 2)\n",
    "\n",
    "hgt_field_northeast = np.roll(hgt_field_north, -a, axis = 2)\n",
    "hgt_field_northwest = np.roll(hgt_field_north, a, axis = 2)\n",
    "hgt_field_southeast = np.roll(hgt_field_south, -a, axis = 2)\n",
    "hgt_field_southwest = np.roll(hgt_field_south, a, axis = 2)\n",
    "\n",
    "hgt_field_west2 = np.roll(hgt_field, b, axis = 2)\n",
    "hgt_field_east2 = np.roll(hgt_field, -b, axis = 2)\n",
    "\n",
    "hgt_field_northeast2 = np.roll(hgt_field_north2, -b, axis = 2)\n",
    "hgt_field_northwest2 = np.roll(hgt_field_north2, b, axis = 2)\n",
    "hgt_field_southeast2 = np.roll(hgt_field_south2, -b, axis = 2)\n",
    "hgt_field_southwest2 = np.roll(hgt_field_south2, b, axis = 2)\n",
    "\n",
    "# This is where the local minima are actually found:      \n",
    "t1n = hgt_field_north - hgt_field\n",
    "t1ne = (hgt_field_northeast - hgt_field) / (2 ** 0.5)\n",
    "t1e = hgt_field_east - hgt_field\n",
    "t1se = (hgt_field_southeast - hgt_field) / (2 ** 0.5)\n",
    "t1s = hgt_field_south - hgt_field\n",
    "t1sw = (hgt_field_southwest - hgt_field) / (2 ** 0.5)\n",
    "t1w = hgt_field_west - hgt_field\n",
    "t1nw = (hgt_field_northwest - hgt_field) / (2 ** 0.5)\n",
    "\n",
    "t2n = hgt_field_north2 - hgt_field\n",
    "t2ne = (hgt_field_northeast2 - hgt_field) / (2 ** 0.5)\n",
    "t2e = hgt_field_east2 - hgt_field\n",
    "t2se = (hgt_field_southeast2 - hgt_field) / (2 ** 0.5)\n",
    "t2s = hgt_field_south2 - hgt_field\n",
    "t2sw = (hgt_field_southwest2 - hgt_field) / (2 ** 0.5)\n",
    "t2w = hgt_field_west2 - hgt_field\n",
    "t2nw = (hgt_field_northwest2 - hgt_field) / (2 ** 0.5)\n",
    "\n",
    "#low_test is a numpy array of True/False values, where a True value corresponds\n",
    "# to a point in the geopotential height field that is lower than each of the 16\n",
    "# surrounding points. The shape of the low_test array corresponds to the shape\n",
    "# of the hgt_field (i.e. time, lat, lon.)\n",
    "low_test = (t1n > 0) & (t1ne > 0) & (t1e > 0) & (t1se > 0) & (t1s > 0) & (t1sw > 0) & (t1w > 0) & (t1nw > 0) & (t2n > hgt_threshold) & (t2ne > hgt_threshold) & (t2e > hgt_threshold) & (t2se > hgt_threshold) & (t2s > hgt_threshold) & (t2sw > hgt_threshold) & (t2w > hgt_threshold) & (t2nw > hgt_threshold)\n",
    "\n",
    "# Now we loop through each of the indentified minima from the previous\n",
    "# step, pull out the 5*5 gridsquare field from the global geopotential\n",
    "# height array, and compute the cyclone metrics with the functions\n",
    "# defined at the top of the program. \n",
    "\n",
    "# It is important to extract a new field of hgt_data covering the whole globe, \n",
    "# to handle local minima identidfied at the edges of the study domain.                        \n",
    "hgt_global_da = xr_obj[gph_variable_name].sel(lev = p_level)\n",
    "hgt_data_global = hgt_global_da.values\n",
    "\n",
    "domain_lats = hgt_global_da['lat'].values\n",
    "domain_lons = hgt_global_da['lon'].values\n",
    "domain_time = hgt_global_da['time'].values\n",
    "\n",
    "lat_inds_global = tuple(np.squeeze(np.where((domain_lats <= lat_range[0]) & (domain_lats >= lat_range[1]))))\n",
    "lat_offset = lat_inds_global[0] # This accounts for the fact that the row index\n",
    "# identified in the variable 'local_minima' refers to the subset region\n",
    "# of the specific band of latitudes being studied. \n",
    "\n",
    "# cyclone_inds is a tuple of three arrays consisting of the time, lat and lon\n",
    "# indices of each of the 'True' values in low_test.\n",
    "cyclone_inds = np.where(low_test)\n",
    "\n",
    "cyclone_tsteps = cyclone_inds[0][:]\n",
    "cyclone_lats = cyclone_inds[1][:]\n",
    "cyclone_lons = cyclone_inds[2][:]\n",
    "    \n",
    "local_minima_loop = range(0,len(cyclone_inds[0]), 1)\n",
    "\n",
    "low_center_count = 0 \n",
    "\n",
    "for l in local_minima_loop:\n",
    "    \n",
    "    lat = cyclone_lats[l] + lat_offset\n",
    "    lon = cyclone_lons[l]\n",
    "    t = cyclone_tsteps[l]\n",
    "    \n",
    "    #print('tstep,lat,lon: ',t , ',', lat, ',', lon)\n",
    "    \n",
    "    # These next tests handle cases of a closed low found where it overlaps the \n",
    "    # International Dateline\n",
    "    if lon < b:\n",
    "        \n",
    "        # Extract the cyclone_field in two parts and concatenate them here\n",
    "        west = hgt_data_global[t, lat - b : lat + b + 1, len(domain_lons) - (b - lon) : ]\n",
    "        east = hgt_data_global[t, lat - b : lat + b + 1, : b + lon + 1]\n",
    "        cyclone_field = np.concatenate((west, east), axis = 1)\n",
    "        \n",
    "    elif lon >= len(domain_lons) - b:\n",
    "        \n",
    "        inv_lon = len(domain_lons) - lon\n",
    "        west = hgt_data_global[t, lat - b : lat + b + 1, len(domain_lons) - (b + inv_lon) : ]\n",
    "        east = hgt_data_global[t, lat - b : lat + b + 1, : b - inv_lon + 1]\n",
    "        cyclone_field = np.concatenate((west, east), axis = 1)\n",
    "    \n",
    "    # This last case is all other lows in the middle of the domain.\n",
    "    else:    \n",
    "\n",
    "        cyclone_field = hgt_data_global[t, lat - b : lat + b + 1, lon - b : lon + b + 1]\n",
    "        \n",
    "    low_result = low_center_test(cyclone_field, hgt_threshold, a, b) # now just modify this line\n",
    "    # and function to incorporate the a and b parameters\n",
    "    \n",
    "    #print(low_result[0])\n",
    "    \n",
    "    if low_result[0]:\n",
    "        \n",
    "        raw_lows_z[low_center_count,0] = l\n",
    "        y,m,d,h = date_from_netcdftime(domain_time[t], time_y0)\n",
    "        raw_lows_z[low_center_count,1] = y\n",
    "        \n",
    "        raw_lows_z[low_center_count,2] = m\n",
    "        raw_lows_z[low_center_count,3] = d\n",
    "        raw_lows_z[low_center_count,4] = h\n",
    "        raw_lows_z[low_center_count,5] = netcdf_time_scale(y,m,d,h,time_y0)\n",
    "                \n",
    "        raw_lows_z[low_center_count,6] = domain_lats[lat]\n",
    "        raw_lows_z[low_center_count,7] = domain_lons[lon]\n",
    "        \n",
    "        raw_lows_z[low_center_count,8] = low_result[1]\n",
    "        raw_lows_z[low_center_count,9] = low_result[2]\n",
    "        raw_lows_z[low_center_count,10] = low_result[3]\n",
    "        \n",
    "        low_center_count = low_center_count + 1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('**** ERROR IN TRACKING SCHEME !!!!!! ****')\n",
    "        print('tstep,lat,lon: ',t , ',', lat, ',', lon)\n",
    "        \n",
    "        continue\n",
    "           \n",
    "xr_obj.close()\n",
    "   \n",
    "#Remove empty rows from 'raw_lows' and save it in a file. \n",
    "ir = (raw_lows_z[:,1] != 0)\n",
    "raw_lows_z = raw_lows_z[ir,:] \n",
    "\n",
    "#ADD THE OPTION TO REMOVE THE MINOR LOW CENTERS HERE\n",
    "if filter_minor_lows:\n",
    "\n",
    "    eliminate_secondary_lows(raw_lows_z, low_filtering_spatial_threshold)\n",
    "\n",
    "output_filename = str(save_path) + 'rawlows_pg_' + str(p_level) + '_' + dataset + '.txt'\n",
    "np.savetxt(output_filename, raw_lows_z, delimiter = ',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b43a633-f11f-445f-b968-725594525b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if filter_minor_lows:\n",
    "\n",
    "    eliminate_secondary_lows(raw_lows_z, low_filtering_spatial_threshold)\n",
    "\n",
    "output_filename = str(save_path) + 'rawlows_pg_' + str(p_level) + '_' + dataset + '.txt'\n",
    "np.savetxt(output_filename, raw_lows_z, delimiter = ',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb8950-4b11-4b0c-a174-d4dc50bbc687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-23.01]",
   "language": "python",
   "name": "conda-env-analysis3-23.01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
