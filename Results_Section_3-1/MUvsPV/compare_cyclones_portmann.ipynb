{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf6e342-d0da-4701-b377-87baac34b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is for comparing a dataset of located weather system centers with the \n",
    "dataset of potential vorticity cutoffs published in Portmann et al. 2021 \n",
    "(https://doi.org/10.5194/wcd-2-507-2021). \n",
    "This code computes how many weather systems are located within the areas of \n",
    "stratospheric potential vorticity (hits), and how many are located outside (misses). \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6333e6-8be5-4f47-b364-c67b8cae80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourxdaily_timestep(y,m,d,h,y0):\n",
    "    \n",
    "    '''This function takes a date in y,m,d,h format and returns the value of the\n",
    "    hours since jan 1, y0 (the time convention of reanalysis). Inputs are integers. \n",
    "    Output is a float.''' \n",
    "    \n",
    "    d0 = datetime(y0,1,1,0) # the datetime object representing 12am, jan 1 of the baseline year of the reanalysis\n",
    "    d = datetime(y,m,d,h) # the datetime object for the day and time being tested\n",
    "    \n",
    "    t = int(4 * (dt.date2num(d) - dt.date2num(d0)) )# number of timesteps since 12am, jan 1 of the baseline year\n",
    "    # dt.date2num gives the number of days since the python datetime time origin. \n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faa93a3e-f441-4e93-b275-1fe9986dabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results = 1\n",
    "\n",
    "years = (1979, 2017)\n",
    "months = (1,12)\n",
    "\n",
    "save_path = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Output_Data/MUvsPV/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e27f09-cf37-4ce5-940f-b05c54e81406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Open the array of Portmann fields and extract the numpy array\n",
    "portmann_dataset_filename = '/g/data/w40/nxg561/Portmann_Cutoffs/300_320/pv_cutoffs_1979-2017.nc'\n",
    "xr_obj_pm = xr.open_dataset(portmann_dataset_filename, engine = 'netcdf4')\n",
    "\n",
    "pm_array = xr_obj_pm['track_id'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5cb2713-bd9f-4129-b480-f1c45d08a563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part computes the relevant offset value for the latitudes of the cyclone database to \n",
    "# correspond to indices of the Portmann's array. \n",
    "pm_lats = xr_obj_pm['latitude'].data\n",
    "lat_offset = pm_lats[0]\n",
    "lat_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc3446e-4f03-4350-b555-47dc2d6df66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset of lows identified by the melbourne uni scheme\n",
    "dataset0_filename = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Input_Data/Acacia_Lows/closed_lows_mu_era5_2017.txt'\n",
    "dataset0 = np.loadtxt(dataset0_filename, delimiter = ',')\n",
    "\n",
    "#loop through the rows of dataset0 and convert the timestep to the rows of the Portmann's array\n",
    "rows = np.shape(dataset0)[0]\n",
    "\n",
    "lat_inds = np.zeros((rows))\n",
    "lon_inds = np.zeros((rows))\n",
    "\n",
    "for row in range(rows):\n",
    "    \n",
    "    dataset0[row,5] = fourxdaily_timestep(int(dataset0[row,1]),int(dataset0[row,2]),int(dataset0[row,3]),int(dataset0[row,4]),1979)\n",
    "\n",
    "    # add an offset to the lat and lon values so they become indices of the Portmann's array\n",
    "    lat_inds[row] =  int(dataset0[row,6]) - lat_offset\n",
    "    lon_inds[row] =  int(dataset0[row,7]) + 180\n",
    "\n",
    "time_inds = dataset0[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffee91d2-5a10-4897-99a2-104c86ee94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the data elements of the Portmann's array at each of the time, lat and lon \n",
    "# points of the lows reccorded in dataset0. The value will be greater than 0 at a point\n",
    "# where Portmann's dataset reccords a pv cutoff. \n",
    "matcharray = pm_array[time_inds.astype(int), lat_inds.astype(int), lon_inds.astype(int)]\n",
    "#print(np.shape(pm_array))\n",
    "#print(np.shape(matcharray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8785ac-ba2c-407e-bf94-87584f1538b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     2     3 ... 43784 43785 43786]\n",
      "[    0     7     8 ... 43781 43782 43783]\n"
     ]
    }
   ],
   "source": [
    "hits_inds = np.where(matcharray > 0)\n",
    "extra_inds = np.where(matcharray == 0)\n",
    "\n",
    "hits = np.shape(hits_inds)[1]\n",
    "extras = np.shape(extra_inds)[1]\n",
    "\n",
    "print(hits_inds[0])\n",
    "print(extra_inds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187c1d88-2472-4d30-ab9c-7417cdc2c5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU Percent matches:\n",
      "0.5789846301413661\n"
     ]
    }
   ],
   "source": [
    "# Compute the percentage of closed lows from dataset0 that match pv cutoffs\n",
    "# in Portmann's dataset.\n",
    "percent_matches = hits / rows\n",
    "print('MU Percent matches:')\n",
    "print(percent_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b00e5a-1393-4b0e-ae74-c04c5b54bc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches per year:\n",
      "650.0512820512821\n",
      "Extras per year:\n",
      "472.6923076923077\n"
     ]
    }
   ],
   "source": [
    "print('Matches per year:')\n",
    "print(hits/ (years[1] - years[0] + 1))\n",
    "print('Extras per year:')\n",
    "print(extras/ (years[1] - years[0] + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aac0a10-2c3f-440d-9ced-52ea43173969",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "\n",
    "    # save the output\n",
    "    np.savetxt(save_path + 'mu_matches.txt', dataset0[hits_inds[0],:], delimiter = ',')\n",
    "    np.savetxt(save_path + 'mu_only.txt', dataset0[extra_inds[0],:], delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f253d-5127-4c0b-a090-5edaeecd93ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-unstable]",
   "language": "python",
   "name": "conda-env-analysis3-unstable-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
