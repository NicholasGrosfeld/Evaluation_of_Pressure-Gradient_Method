{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d900989-75a7-42a1-8aa8-533b6322fb57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This code is for comparing two datasets of located weather events that should\n",
    "in theory be identical. This code computes how many weather events are recorded by\n",
    "both datasets, and how many events are recorded in one dataset but not the other. \n",
    "\"\"\"\n",
    "import numpy as np\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020a92ed-e500-4f2f-a179-8ee399f45980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_results = 0\n",
    "\n",
    "spatial_tolerance = 5\n",
    "years = (1979, 2017)\n",
    "\n",
    "save_path = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Output_Data/PGvsMU/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6784ca6e-8825-4771-b678-a85ca8a4dc88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Load the datasets as numpy arrays\n",
    "dataset0_filename = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Input_Data/Nick_Lows/closed_lows_pg_era5_2017.txt'\n",
    "dataset0 = np.loadtxt(dataset0_filename, delimiter = ',')\n",
    "\n",
    "dataset1_filename = '/home/561/nxg561/00_Tracking_Scheme_Comparison/Input_Data/Acacia_Lows/closed_lows_mu_era5_2017.txt'\n",
    "dataset1 = np.loadtxt(dataset1_filename, delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1fc46-e0c4-4141-a1c4-5c532754f976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n"
     ]
    }
   ],
   "source": [
    "# This next code loops through each low entry in the first dataset and compares\n",
    "# its timestep, latitude and longitude to each low entry in the second dataset.\n",
    "# Matching lows from each dataset are appended to a list. \n",
    "\n",
    "# A scalar number of the total number of matching cyclones\n",
    "matches = 0\n",
    "\n",
    "# Empty lists that will hold the dataset rows of each of the matching cyclones\n",
    "dataset0_matches = []\n",
    "dataset1_matches = []\n",
    "\n",
    "# Loop through each year\n",
    "for year in range(years[0], years[1] + 1):\n",
    "\n",
    "    print(year)\n",
    "\n",
    "    # Extract the cyclone entries for that year from both datasets\n",
    "    dataset0_y = dataset0[(dataset0[:,1] == year), :] \n",
    "    dataset1_y = dataset1[(dataset1[:,1] == year), :]  \n",
    "    \n",
    "    len0 = np.shape(dataset0_y)[0]\n",
    "    len1 = np.shape(dataset1_y)[0]\n",
    "\n",
    "    # Loop through the cyclone entries in dataset0\n",
    "    for i0 in range(0,len0):\n",
    "\n",
    "        # Loop through the cyclone entries in dataset1\n",
    "        for i1 in range(0,len1):\n",
    "            \n",
    "            cyclone0 = dataset0_y[i0, :]\n",
    "            cyclone1 = dataset1_y[i1, :]\n",
    "\n",
    "            # Test if the time steps of the two cyclone entries match\n",
    "            tstepmatch = cyclone0[5] == cyclone1[5]\n",
    "\n",
    "            # Test if the lat and lon entries of the two cyclone entries are within the spatial tollerance\n",
    "            latmatch = np.abs(cyclone0[6] - cyclone1[6]) <= spatial_tolerance\n",
    "            lonmatch = np.abs(cyclone0[7] - cyclone1[7]) <= spatial_tolerance\n",
    "            \n",
    "            if tstepmatch and latmatch and lonmatch:\n",
    "                \n",
    "                matches = matches + 1\n",
    "                \n",
    "                #np.copy is needed here becayse the numpy arrays dataset0 and \n",
    "                #dataset1 are indexed with direct integers (not conditionals)\n",
    "                dataset0_matches.append(np.copy(cyclone0))\n",
    "                dataset1_matches.append(np.copy(cyclone1))\n",
    "                \n",
    "                # The dataset0_y and dataset1_y are overwritten with np.nan to\n",
    "                # prevent a system being matched twice. \n",
    "                dataset0_y[i0,:] = np.nan\n",
    "                dataset1_y[i1,:] = np.nan\n",
    "                \n",
    "                continue\n",
    "\n",
    "    # The cyclone entries for each year are written back into the original arrays (both the matched\n",
    "    # cyclones that are indicated by nans, and the extra cyclone entries that are left over) so the\n",
    "    # lists of extra systems can be obtained. \n",
    "    dataset0[(dataset0[:,1] == year), :] = dataset0_y\n",
    "    dataset1[(dataset1[:,1] == year), :] = dataset1_y\n",
    "                          \n",
    "print('Number of matches: ', matches)\n",
    "matches_per_year = matches / (years[1] - years[0] + 1)\n",
    "print('Matches per year: ', matches_per_year)\n",
    "\n",
    "# Format the output as a numpy array and save to a .txt file here:\n",
    "dataset0_matches_array = np.array(dataset0_matches)\n",
    "dataset1_matches_array = np.array(dataset1_matches)\n",
    "\n",
    "# Get the extra systems from each of the two datasets\n",
    "ir0 = np.isnan(dataset0[:,0])\n",
    "dataset0_only = dataset0[(ir0 == False),:]\n",
    "\n",
    "ir1 = np.isnan(dataset1[:,0])\n",
    "dataset1_only = dataset1[(ir1 == False),:]\n",
    "\n",
    "if save_results == 1:\n",
    "\n",
    "    output_filename = str(save_path) + 'pg_matches.txt'\n",
    "    np.savetxt(output_filename, dataset0_matches_array, delimiter = ',') \n",
    "    \n",
    "    output_filename = str(save_path) + 'mu_matches.txt'\n",
    "    np.savetxt(output_filename, dataset1_matches_array, delimiter = ',') \n",
    "    \n",
    "    output_filename = str(save_path) + 'pg_only.txt'\n",
    "    np.savetxt(output_filename, dataset0_only, delimiter = ',') \n",
    "    \n",
    "    output_filename = str(save_path) + 'mu_only.txt'\n",
    "    np.savetxt(output_filename, dataset1_only, delimiter = ',') \n",
    "    \n",
    "    #output_filename = str(save_path) + 'mu_all.txt'\n",
    "    #np.savetxt(output_filename, dataset0_all, delimiter = ',') \n",
    "    \n",
    "    #output_filename = str(save_path) + 'g21_all.txt'\n",
    "    #np.savetxt(output_filename, dataset1_all, delimiter = ',') \n",
    " \n",
    "# Compute and display the Critical Success Index\n",
    "csi = len(dataset0_matches_array) / (len(dataset0_matches_array) + len(dataset0_only) + len(dataset1_only))\n",
    "\n",
    "print('CSI: ', csi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37fa01ca-3fc7-4016-9b05-7795b4fbab8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G21 lows only\n",
      "5314\n",
      "G21 lows only per year:\n",
      "136.25641025641025\n"
     ]
    }
   ],
   "source": [
    "g21_only = np.shape(dataset0_only)[0]\n",
    "print('G21 lows only')\n",
    "print(g21_only)\n",
    "\n",
    "g21_only_year = g21_only / (years[1] - years[0] + 1)\n",
    "print('G21 lows only per year:')\n",
    "print(g21_only_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c68d6ed6-22de-4ea3-9ad8-f3c47ad6b1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU lows only\n",
      "5859\n",
      "MU lows only per year:\n",
      "150.23076923076923\n"
     ]
    }
   ],
   "source": [
    "mu_only = np.shape(dataset1_only)[0]\n",
    "print('MU lows only')\n",
    "print(mu_only)\n",
    "\n",
    "mu_only_year = mu_only / (years[1] - years[0] + 1)\n",
    "print('MU lows only per year:')\n",
    "print(mu_only_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "104cc3d5-6cea-4fff-9e76-b8a47280d988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG Percent:  87.71102169187364\n",
      "MU Percent:  86.61931623541234\n"
     ]
    }
   ],
   "source": [
    "print('PG Percent: ', 100 * matches / (matches + g21_only))\n",
    "print('MU Percent: ', 100 * matches / (matches + mu_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e40f95-bb0d-4482-836e-f0642bfd33af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-23.01]",
   "language": "python",
   "name": "conda-env-analysis3-23.01-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
